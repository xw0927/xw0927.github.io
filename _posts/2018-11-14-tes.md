---
layout: post
date: 2018-11-14
title: Basic/Premium Car Classification
tags: [Image_Recognition, Deep_Learning]
---

Today, there are dozens of deep learning tools available to solve complex problems such as image recognition or speech recognition. Two most popular frameworks are TensorFlow and Keras. In this post, I will talk about techniques that I used in my first image recognition project: Premium car classification with these two popular frameworks. 

![overview](/images/overview.png)

### TensorFlow and Keras framework

**TensorFlow** is developed & maintained by Google Brain Team. TensorFlow is low level library which is written with a Python API over a C/C++ engine and provide much more flexibility as compared to Keras.

**Keras** is a high level library which is written in Python by Francois. It is high level API built on TensorFlow. Keras is simple, easy to use and learn as compared to TensorFlow which allows you quickly experiment with deep learning without spending a lot of time to learn the details of coding or doing math behind the layers.

### Dataset

Input: Car image,  h * w * d (h = Height, w = Width, d = Dimension),  240* 240*3
Total ~3k car images are scrapped from car rental website with python scrapy framework. 52.5% of car images are labelled as basic-0 (eg: honda city, toyota vios, mazida) whereas 47.5% are labelled as premium-1 (eg: Audi, BMW, mercedes, mini cooper) as shown in barplot below. This dataset is quite balanced.

![dataset](/images/dataset.png)

In order to prevent model from over-fitting, I used Keras ImageDataGenerator module to augment images by flipping, rotating, zoom-in original images. This can helps the model to classify images more accurately regardless of the perspective from which it is displayed.  Here is snapshot of augmented images: 

Original image:
![original_image](/images/orignal_image.png)

Augmented images with setting:
- Rotation range: 40
- Width shift range: 0.2
- Height shift range: 0.2
- Shear range: 0.2
- Zoom range: 0.2
- Horizontal flip: True

Result:
![augmented_image](/images/augmented_image.png)

###Convolutional Neural network
In deep learning, convolutional neural network (CNN) is widely used for image recognition. A convolutional neural network can be divided into two components:
![CNN](/images/CNN.png)
*CNN architecture, source: [Link](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148). Here is detailed explanation for each block by Daphne: [Link](https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050)

Output: binary label ( 1: Premium , 0: Basic )

1. Features extraction
- Convolutional + Relu : Convolution is performed by sliding the filter over image to create feature map. 
- Pooling : To reduce number of parameters and retains important information

2. Classification- to compute the score for each of our classes
- Flatten: To convert 3D to 1D data as FC layer only accept 1D data
- Fully Connected (FC):  To learn non-linear combination of features of images
- Softmax: To classify premium/basic with probability value between 0 and 1

The network is trained by using a set of labelled images which allows it to optimize weights of filters through back propagation. Fully connected layers is updated to help the network to classify whereas convolution layers get updated to help the network to learn the feature. Performance of deep learning neural network is judged by training accuracy and validation accuracy.

### 1. Baseline Model
![1st_model](/images/model.png)

For this project, baseline model,1st model is constructed by using 3 CNN layers, 2 Dense layers.The training/ validation dataset ratio is 7:3. After 50 epoch, the baseline model was able to reach training accuracy of 0.5842 and validation accuracy of 0.570. It took 4 hours to complete 50 epoch. 
![1st_model](/images/1st_model.png)

### 2. Alexnet Model

![alexnet](/images/alextnet.png)


To enhance accuracy, I applied transfer learning technique. AlexNet network is used as my 2nd model. All of early layers of the network are frozen whereas last layers are removed to match desired output. Input images are resized into 227 x 227 x 3 because that is size required by AlexNet and then the images are mapped into feature vectors with pre-trained weight of AlexNet. The pre-trained weight of AlexNet can be download from internet. Then the feature vectors (total 4096 features) are feed to my own dense/softmax layer to make prediction. The advantages of this technique is to save computing resources, time and a better performance can be obtained even with small dataset. 
![1st_output](/images/1st_output.png)

### 3. VGGnet-16 Model

![VGGnet](/images/VGGnet.png)
source: [Link](https://www.researchgate.net/figure/VGG-16-network-block-diagram-It-contains-16-CONV-FC-13-convolutions-and-3-fully_317573733)
Third model, 3rd I used is VGG-Net16. Similar to the first model, I removed last layers and created own softmax function. Instead of only training last few layers, this time I train whole network. The weight of VGGNet-16 is used to initialize the network. The VGGnet-16 model was able to reach training accuracy of 0.8412 and validation accuracy of 0.8336 after 20 epoch. It took ~36 hours to complete 20 epoch. 

![VGGnet](/images/VGGnet_result.png)

### Summary of models

![comparison](/images/comparison.png)

Let’s see what each layer in VGGnet-16 network detect & output of each layers to understand how and what the network is learning for premium(left) or basic car(right)! 

Test input image:
![result](/images/result.png)


Filter layer is obtained by running Gradient Descent on the blank input images to maximize the activation of specific filter in specific layer. For example, filter of layer 1 (block1_conv1) encode direction and color. 

1. Filter of Layer 1 (block1_conv1)

![filter_info](/images/filter_info.png)
![1st_filter](/images/1st_filter.png)

Output for 1st layer
![1st_output](/images/1st_output.png)

As the network go deeper, the filters of network tends to capture more complex object.For example, layer 3 (block3_conv1) captures line and texture of image: 

![3rd_layer](/images/3rd_layer.png)

Output for 3rd layer

![3rd_output1](/images/3rd_output1.png)
![3rd_output2](/images/3rd_output2.png)

From images above, it is pretty clear that each filters/layers captures different characteristics of the image. First filter is used to recognize direction and color,  then shapes, texture, and then more complex features of input images.

So how to identify which features are more likely to contribute to the desired outcome? To determine it, I have done few experiments by providing preprocessed images or each of possible cases to the model and see how accuracy drops. 

1.Logo analysis

Looking at the table above, we can see that body shape impact the performance greatly. 

2. Color, orientation of car

The prediction of premium (gray scale) is worse than RGB image, therefore, Color information does help to improve model performance performance because in grayscale, some details is lost (eg: edge information). 

3. Background analysis

With enhanced contrast of image, the model would spot object much easier. As a result, a higher score is obtained on high-contrast image. For example, white object on black background or black object on white background give lower intensity & contrast is high. 

4. Overlay of premium and basic car images

From the above result, we can see that the model couldn’t classify the images properly, even as a human being, we also very hard to tell it is premium or basic. 

### Flask Application
The VGGnet-16 model is saved and then a Flask app is created. After making sure the Flask app is running fine on local host, then the app is deployed to PythonAnywhere web server so that everyone could access it. I use Filezilla to upload those files with size > 100MB, because you only can upload file size <100MB via upload button.

![web](/images/web.png)

### Fun stuff
1. Google DeepDream
Another fun things to do is to pass through the image to Google DeepDream script to generate trippy image. The cool thing about this is you never know what you will get in the end. 
For example: Test image
![google](/images/google_deep_o.png)

Trippy Images
![google_deep_dream](/images/google_deep.png)

2. Object localization (object detection packages)
![object_localization](/images/object_localization.png)

3. Image Captioning (Microsoft AI caption bot)
![image_captioning](/images/image_captioning.png)




---


